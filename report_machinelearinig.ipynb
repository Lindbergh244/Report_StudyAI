{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# レポート(機械学習)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第1章：線形回帰モデル\n",
    "&nbsp; &nbsp; &nbsp; 回帰問題とは、ある入力(数値)から出力(連続値)を予測する問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-1.データの形式\n",
    "- 入力は、m次元のベクトル(m=1の場合は、スカラ)\t\n",
    "- 入力ベクトルの各要素は、説明変数(または特徴量)と呼ぶ\t\n",
    "- 出力はスカラー値(目的変数と呼ぶ)\n",
    "\n",
    "&nbsp; &nbsp; &nbsp;  $ w = (w_1,w_2,\\cdots,w_m)^T  \\in \\mathbb{ R }^m$  &nbsp; &nbsp; &nbsp;   $y \\in \\mathbb{ R }^1$ \n",
    "\n",
    "&nbsp; &nbsp; &nbsp;  線形回帰は直線で、非線形回帰は曲線で近似したもの"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-1.線形回帰\n",
    "- 教師あり学習(回答付きデータから学習)\t\n",
    "- 入力とm次元パラメータの線形結合を出力するモデル\t\n",
    "\n",
    "&nbsp; &nbsp; &nbsp;  $ w = (w_1,w_2,･･･,w_m)^T  \\in \\mathbb{ R }^m$\n",
    "\n",
    "&nbsp; &nbsp; &nbsp;  $ \\hat{y} = w^Tx + b =\\displaystyle \\sum_{ j = 1 }^{ m } w_j x_j +b  $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-2.線形結合\n",
    "- 入力ベクトルと道のパラメータの各要素を掛け算し、足し合わせたもの\n",
    "- 入力ベクトルとの線形結合に加え、切片も足し合わせる\n",
    "- 出力は1次元(スカラ)となる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-3.線形回帰モデルのパラメータ\n",
    "- 特徴量が予測値に対し、どのように影響を与えるのかを決定させる重みの集合\n",
    "- 正の(負の)重みをつける場合、その特徴量の値を増加させると、予測の値が増加(減少)\n",
    "- 重みが大きければ(0であれば)、その特徴量は予測に大きな影響力を持つ(全く影響しない)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-4.線形単(m=1)回帰モデル(データへの仮定)\n",
    "&nbsp; &nbsp; &nbsp;  データは回帰 <span style=\"color: red; \">**直線** </span>に誤差が加わり、観測されていると仮定\n",
    "\n",
    "&nbsp; &nbsp; &nbsp; $y = w_0 + w_1 x_1 +  \\varepsilon $  &nbsp; &nbsp;$y:目的関数 \\quad w_0:切片 \\quad w_1:回帰係数 \\quad x_1:説明変数 \\quad \\varepsilon:誤差 $\n",
    "\n",
    "&nbsp; &nbsp; &nbsp;未知パラメータ(切片、回帰係数)は、最小二乗法により推定"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-5.線形回帰モデル(行列表現)\n",
    "&nbsp; &nbsp; &nbsp;連立方程式は、行列(転置行列)で表現"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-6.線形単(m=多次元)回帰モデル(データへの仮定)\n",
    "&nbsp; &nbsp; &nbsp;  データは回帰 <span style=\"color: red; \">**直面** </span>に誤差が加わり、観測されていると仮定\n",
    "\n",
    "&nbsp; &nbsp; &nbsp;次元が増えている以外、m=1の場合と同じ"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-1.モデルの評価\n",
    "- データの分割\n",
    "    - データを学習用と検証用データに分割する\n",
    "    - 学習データで学習し、検証用データで汎化性能(モデルの精度)を測定する"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-1.モデル(パラメータ)の推定\n",
    "- 平均二乗誤差(MSE)\n",
    "    - データとモデル出力の二乗誤差\n",
    "    - 小さいほど直線とデータの距離が近い\n",
    "    \n",
    "    $ MSE_{train} = \\frac{ 1 }{ n_{train} } \\displaystyle \\sum_{ i = 1 }^{ n_{train} } (\\hat{y}_i^{(train)} - y_i^{(train)})^2 $\n",
    "\n",
    "- 最小二乗法\n",
    "    - 学習データの平均二乗誤差を最小とする\n",
    "    - 学習データの平均二乗誤差の最小化は、その勾配が0になる点\n",
    "    \n",
    "    $ \\frac{ \\partial }{ \\partial w } MSE_{train} = 0 $\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
